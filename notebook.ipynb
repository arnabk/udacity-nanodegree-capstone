{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker import KMeans\n",
    "import os\n",
    "import numpy as np\n",
    "from helper.utils import create_dir, clustering, save_data\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from glob import glob\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "threshold = .4\n",
    "BUY = 1\n",
    "SELL = 2\n",
    "NONE = 3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "local_data_folder = './data'\n",
    "prefix = \"udacity-capstone-project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_dir(dir):\n",
    "  os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "# Generate clusters for data\n",
    "def clustering(data, kmeans_predictor):\n",
    "    clustering_result = kmeans_predictor.predict(pd.DataFrame(data).astype('float32').values)\n",
    "    clustering_result = list(map(lambda x:x.label[\"closest_cluster\"].float32_tensor.values[0], clustering_result))\n",
    "\n",
    "    assert len(clustering_result) == len(data), \"Length mis-match with clustering and input data\"\n",
    "\n",
    "    cluster_category = pd.DataFrame(clustering_result, columns=[\"Cluster\"])\n",
    "    x_train_with_cluster = pd.concat([pd.DataFrame(data), cluster_category], axis=1)\n",
    "    return cluster_category\n",
    "\n",
    "# save data to local dir\n",
    "def save_data(cluster_data, folder_name):\n",
    "    Y = cluster_data[[\"Label\"]]\n",
    "    X = cluster_data.drop(columns=[\"Label\"])\n",
    "    create_dir(local_data_folder + '/s3/' + folder_name)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.33, random_state=1, shuffle=True)\n",
    "    pd.concat([pd.DataFrame(y_train), pd.DataFrame(x_train)], axis=1)\\\n",
    "        .to_csv(local_data_folder + '/s3/' + folder_name + '/train.csv', header=False, index=False)\n",
    "    pd.concat([pd.DataFrame(y_test), pd.DataFrame(x_test)], axis=1)\\\n",
    "        .to_csv(local_data_folder + '/s3/' + folder_name + '/validation.csv', header=False, index=False)\n",
    "        \n",
    "def generate_NN_predictor(ticker):\n",
    "    s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/data/{}/train.csv'\\\n",
    "                                        .format(bucket, prefix, ticker), content_type='text/csv')\n",
    "    s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/data/{}/validation.csv'\\\n",
    "                                             .format(bucket, prefix, ticker), content_type='text/csv')\n",
    "    estimator = PyTorch(entry_point='train.py',\n",
    "                        source_dir='pytorch', # this should be just \"source\" for your code\n",
    "                        role=role,\n",
    "                        framework_version='1.0',\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type='ml.c4.xlarge',\n",
    "                        sagemaker_session=sagemaker_session,\n",
    "                        hyperparameters={\n",
    "                            'input_dim': 26,  # num of features\n",
    "                            'hidden_dim': 260,\n",
    "                            'output_dim': 1,\n",
    "                            'epochs': 200 # could change to higher\n",
    "                        })\n",
    "    estimator.fit({ 'train': s3_input_train, 'validation': s3_input_validation })\n",
    "    predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")\n",
    "    return predictor\n",
    "\n",
    "def generate_random_direction():\n",
    "    rand_val = random.random()\n",
    "    direction = NONE\n",
    "    if rand_val >= .7:\n",
    "        direction = BUY\n",
    "    elif rand_val <= .3:\n",
    "        direction = SELL\n",
    "    return direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process(ticker):\n",
    "    df = pd.read_pickle('{}/{}.{}'.format(local_data_folder, ticker, 'pkl'))\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop(columns=[\"Date\"], inplace=True)\n",
    "    df.loc[df.Label >= threshold, 'direction'] = BUY\n",
    "    df.loc[df.Label <= -threshold, 'direction'] = SELL\n",
    "    df.loc[(df.Label < threshold) & (df.Label > -threshold), 'direction'] = NONE\n",
    "\n",
    "    # Normalize\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    Y_df = pd.DataFrame(df[\"Label\"]).astype('float64')\n",
    "    X_df = df.drop(columns=[\"Label\"]).astype('float64')\n",
    "\n",
    "    X = scaler.fit_transform(X_df)\n",
    "    Y = scaler.fit_transform(Y_df)\n",
    "\n",
    "    X[:, X.shape[1] - 1] = X_df[\"direction\"].to_numpy()\n",
    "\n",
    "    #### split data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.33, random_state=1, shuffle=True)\n",
    "\n",
    "    # clustering\n",
    "    s3_output_folder = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "    kmeans = KMeans(role=role,\n",
    "                train_instance_count=1,\n",
    "                train_instance_type=\"ml.m4.xlarge\",\n",
    "                output_path=s3_output_folder,\n",
    "                k=3)\n",
    "\n",
    "    # Remove direction column and train\n",
    "    kmeans.fit(kmeans.record_set(x_train[:, 0:x_train.shape[1] - 1].astype('float32')))\n",
    "\n",
    "    # deploy\n",
    "    print(\"Deploying model\", kmeans.model_data)\n",
    "    kmeans_predictor = kmeans.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")\n",
    "\n",
    "    create_dir('{}/s3/{}'.format(local_data_folder, ticker))\n",
    "\n",
    "    '''\n",
    "        Label = Change in price(+ve, -ve, none)\n",
    "        Direction = BUY, SELL, NONE\n",
    "        Cluster = cluster_0, cluster_1, cluster_2\n",
    "    '''\n",
    "    # train data\n",
    "    y_train_df = pd.DataFrame(y_train, columns=[\"Label\"])\n",
    "    x_train_df = pd.DataFrame(x_train, columns=['col-{}'.format(i) for i in range(x_train.shape[1] - 1)] + [\"direction\"])\n",
    "    dataset_with_cluster = pd.concat([y_train_df.astype(\"float32\"), x_train_df.astype(\"float32\"),\\\n",
    "            clustering(x_train_df.drop(columns=[\"direction\"]).astype('float32').values, kmeans_predictor)\n",
    "        ], axis=1)\n",
    "    dataset_with_cluster.to_csv('{}/s3/{}/all-train.csv'.format(local_data_folder, ticker), header=True, index=False)\n",
    "\n",
    "    # test data\n",
    "    y_test_df = pd.DataFrame(y_test, columns=[\"Label\"])\n",
    "    x_test_df = pd.DataFrame(x_test, columns=['col-{}'.format(i) for i in range(x_test.shape[1] - 1)] + ['direction'])\n",
    "    pd.concat([y_test_df.astype(\"float32\"), x_test_df.astype(\"float32\")], axis=1)\\\n",
    "        .to_csv('{}/s3/{}/all-test.csv'.format(local_data_folder, ticker), header=True, index=False)\n",
    "\n",
    "    # clean clustering end point\n",
    "    kmeans_predictor.delete_endpoint(kmeans_predictor.endpoint)\n",
    "\n",
    "    all_test_pred = pd.read_csv(\"{}/s3/{}/all-test.csv\".format(local_data_folder, ticker))\n",
    "    all_train_pred = pd.read_csv(\"{}/s3/{}/all-train.csv\".format(local_data_folder, ticker))\n",
    "\n",
    "    cluster0_df = dataset_with_cluster[dataset_with_cluster[\"Cluster\"] == 0].drop(columns=[\"Cluster\"])\n",
    "    save_data(cluster0_df.drop(columns=[\"direction\"]), ticker)\n",
    "    sagemaker_session.upload_data(path=local_data_folder + '/s3/' + ticker, bucket=bucket, key_prefix=prefix + '/data/' + ticker)\n",
    "    estimator = generate_NN_predictor(ticker)\n",
    "    all_test_pred[\"cluster0_pred\"] = estimator.predict(all_test_pred.drop(columns=[\"Label\", \"direction\"]).astype('float32').values)\n",
    "    all_train_pred[\"cluster0_pred\"] = estimator.predict(all_train_pred.drop(columns=[\"Label\", \"direction\", \"Cluster\"]).astype('float32').values)\n",
    "    estimator.delete_endpoint(estimator.endpoint)\n",
    "\n",
    "    cluster1_df = dataset_with_cluster[dataset_with_cluster[\"Cluster\"] == 1].drop(columns=[\"Cluster\"])\n",
    "    save_data(cluster1_df.drop(columns=[\"direction\"]), ticker)\n",
    "    sagemaker_session.upload_data(path=local_data_folder + '/s3/' + ticker, bucket=bucket, key_prefix=prefix + '/data/' + ticker)\n",
    "    estimator = generate_NN_predictor(ticker)\n",
    "    all_test_pred[\"cluster1_pred\"] = estimator.predict(all_test_pred.drop(columns=[\"Label\", \"direction\", \"cluster0_pred\"]).astype('float32').values)\n",
    "    all_train_pred[\"cluster1_pred\"] = estimator.predict(all_train_pred.drop(columns=[\"Label\", \"direction\", \"Cluster\", \"cluster0_pred\"]).astype('float32').values)\n",
    "    estimator.delete_endpoint(estimator.endpoint)\n",
    "\n",
    "    cluster2_df = dataset_with_cluster[dataset_with_cluster[\"Cluster\"] == 2].drop(columns=[\"Cluster\"])\n",
    "    save_data(cluster2_df.drop(columns=[\"direction\"]), ticker)\n",
    "    sagemaker_session.upload_data(path=local_data_folder + '/s3/' + ticker, bucket=bucket, key_prefix=prefix + '/data/' + ticker)\n",
    "    estimator = generate_NN_predictor(ticker)\n",
    "    all_test_pred[\"cluster2_pred\"] = estimator.predict(all_test_pred.drop(columns=[\"Label\", \"direction\", \"cluster0_pred\", \"cluster1_pred\"]).astype('float32').values)\n",
    "    all_train_pred[\"cluster2_pred\"] = estimator.predict(all_train_pred.drop(columns=[\"Label\", \"direction\", \"Cluster\", \"cluster0_pred\", \"cluster1_pred\"]).astype('float32').values)\n",
    "    estimator.delete_endpoint(estimator.endpoint)\n",
    "\n",
    "    os.remove(local_data_folder + '/s3/' + ticker + '/train.csv')\n",
    "    os.remove(local_data_folder + '/s3/' + ticker + '/validation.csv')\n",
    "\n",
    "    all_buys = pd.DataFrame([cluster0_df[cluster0_df['direction'] == BUY].shape[0],\n",
    "            cluster1_df[cluster1_df['direction'] == BUY].shape[0],\n",
    "            cluster2_df[cluster2_df['direction'] == BUY].shape[0]], columns=[\"BUY\"], index=[\"cluster0_pred\", \"cluster1_pred\", \"cluster2_pred\"])\n",
    "\n",
    "    all_sells = pd.DataFrame([cluster0_df[cluster0_df['direction'] == SELL].shape[0],\n",
    "            cluster1_df[cluster1_df['direction'] == SELL].shape[0],\n",
    "            cluster2_df[cluster2_df['direction'] == SELL].shape[0]], columns=[\"SELL\"], index=[\"cluster0_pred\", \"cluster1_pred\", \"cluster2_pred\"])\n",
    "\n",
    "    all_nones = pd.DataFrame([cluster0_df[cluster0_df['direction'] == NONE].shape[0],\n",
    "            cluster1_df[cluster1_df['direction'] == NONE].shape[0],\n",
    "            cluster2_df[cluster2_df['direction'] == NONE].shape[0]], columns=[\"NONE\"], index=[\"cluster0_pred\", \"cluster1_pred\", \"cluster2_pred\"])\n",
    "\n",
    "    cluster_selection_df = pd.concat([all_buys, all_sells, all_nones], axis=1)\n",
    "\n",
    "\n",
    "    cluster_selection_index = cluster_selection_df.index\n",
    "    buy_cluster_name = cluster_selection_index[cluster_selection_df['BUY'].values.argmax()]\n",
    "    sell_cluster_name = cluster_selection_index[cluster_selection_df.drop(index=[buy_cluster_name])['SELL'].values.argmax()]\n",
    "    none_cluster_name = cluster_selection_index[cluster_selection_df.drop(index=[buy_cluster_name, sell_cluster_name])['NONE'].values.argmax()]\n",
    "\n",
    "    # Generate selected-cluster column based on max(cluster0, cluster1, cluster2)\n",
    "    all_test_pred[\"selected-cluster\"] = all_test_pred[[\"cluster0_pred\", \"cluster1_pred\", \"cluster2_pred\"]].idxmax(axis=1)\n",
    "    all_train_pred[\"selected-cluster\"] = all_train_pred[[\"cluster0_pred\", \"cluster1_pred\", \"cluster2_pred\"]].idxmax(axis=1)\n",
    "\n",
    "    # convert selected-cluster to BUY, SELL, NONE\n",
    "    all_test_pred.loc[all_test_pred[\"selected-cluster\"] == buy_cluster_name, \"prediction\"] = BUY\n",
    "    all_test_pred.loc[all_test_pred[\"selected-cluster\"] == sell_cluster_name, \"prediction\"] = SELL\n",
    "    all_test_pred.loc[all_test_pred[\"selected-cluster\"] == none_cluster_name, \"prediction\"] = NONE\n",
    "\n",
    "    all_train_pred.loc[all_train_pred[\"selected-cluster\"] == buy_cluster_name, \"prediction\"] = BUY\n",
    "    all_train_pred.loc[all_train_pred[\"selected-cluster\"] == sell_cluster_name, \"prediction\"] = SELL\n",
    "    all_train_pred.loc[all_train_pred[\"selected-cluster\"] == none_cluster_name, \"prediction\"] = NONE\n",
    "\n",
    "    # Bench mark results\n",
    "    all_test_pred[\"random-prediction\"] = [generate_random_direction() for _ in range(all_test_pred.shape[0])]\n",
    "    all_train_pred[\"random-prediction\"] = [generate_random_direction() for _ in range(all_train_pred.shape[0])]\n",
    "\n",
    "\n",
    "    all_test_pred.to_csv('{}/s3/{}/all-test-pred.csv'.format(local_data_folder, ticker), index=None)\n",
    "    all_train_pred.to_csv('{}/s3/{}/all-train-pred.csv'.format(local_data_folder, ticker), index=None)\n",
    "    cluster_selection_df.to_csv('{}/s3/{}/cluster-selection.csv'.format(local_data_folder, ticker), index=None)\n",
    "\n",
    "    # test accuracy\n",
    "    test_accuracy = accuracy_score(all_test_pred[\"direction\"], all_test_pred[\"prediction\"], normalize=True)\n",
    "    benchmark_test_accuracy = accuracy_score(all_test_pred[\"direction\"], all_test_pred[\"random-prediction\"], normalize=True)\n",
    "    print('Test accuracy:', test_accuracy, \", Benchmark:\", benchmark_test_accuracy)\n",
    "\n",
    "    # train accuracy\n",
    "    train_accuracy = accuracy_score(all_train_pred[\"direction\"], all_train_pred[\"prediction\"], normalize=True)\n",
    "    benchmark_train_accuracy = accuracy_score(all_train_pred[\"direction\"], all_train_pred[\"random-prediction\"], normalize=True)\n",
    "    print('Train accuracy:', train_accuracy, \", Benchmark:\", benchmark_train_accuracy)\n",
    "\n",
    "    accuracy_df = pd.DataFrame([ticker, test_accuracy, benchmark_test_accuracy, train_accuracy, benchmark_train_accuracy]).T\n",
    "    accuracy_df.columns = [\"ticker\", \"test_accuracy\", \"benchmark_test_accuracy\", \"train_accuracy\", \"benchmark_train_accuracy\"]\n",
    "\n",
    "    accuracy_file = \"{}/accuracy.csv\".format(local_data_folder)\n",
    "    header = not os.path.exists(accuracy_file)\n",
    "    accuracy_df.to_csv(accuracy_file, mode=\"a\", header=header, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!2020-02-22 22:54:25 Starting - Starting the training job...\n",
      "2020-02-22 22:54:26 Starting - Launching requested ML instances...\n",
      "2020-02-22 22:55:23 Starting - Preparing the instances for training......\n",
      "2020-02-22 22:56:24 Downloading - Downloading input data\n",
      "2020-02-22 22:56:24 Training - Downloading the training image...\n",
      "2020-02-22 22:56:43 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-02-22 22:56:44,588 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-02-22 22:56:44,590 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-22 22:56:44,602 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-02-22 22:56:44,907 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-02-22 22:56:45,146 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-02-22 22:56:45,146 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-02-22 22:56:45,146 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-02-22 22:56:45,146 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/08/ec/b5dd8cfb078380fb5ae9325771146bccd4e8cad2d3e4c72c7433010684eb/pandas-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2019.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->-r requirements.txt (line 1)) (1.12.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-a_q9aen1/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, pandas, train\n",
      "  Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.16.4\u001b[0m\n",
      "\u001b[34m  Found existing installation: pandas 0.24.2\n",
      "    Uninstalling pandas-0.24.2:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled pandas-0.24.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed numpy-1.18.1 pandas-1.0.1 train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-02-22 22:56:55,238 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-22 22:56:55,250 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"input_dim\": 26,\n",
      "        \"hidden_dim\": 260,\n",
      "        \"epochs\": 200,\n",
      "        \"output_dim\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-02-22-22-54-24-850\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-825285592721/sagemaker-pytorch-2020-02-22-22-54-24-850/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":200,\"hidden_dim\":260,\"input_dim\":26,\"output_dim\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-825285592721/sagemaker-pytorch-2020-02-22-22-54-24-850/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":200,\"hidden_dim\":260,\"input_dim\":26,\"output_dim\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2020-02-22-22-54-24-850\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-825285592721/sagemaker-pytorch-2020-02-22-22-54-24-850/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"200\",\"--hidden_dim\",\"260\",\"--input_dim\",\"26\",\"--output_dim\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_DIM=26\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=260\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=200\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIM=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 200 --hidden_dim 260 --input_dim 26 --output_dim 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mGet data loader.\u001b[0m\n",
      "\u001b[34mEpoch: 1, Loss: 0.6831454038619995\u001b[0m\n",
      "\u001b[34mEpoch: 2, Loss: 0.6817636966705323\u001b[0m\n",
      "\u001b[34mEpoch: 3, Loss: 0.6815218210220337\u001b[0m\n",
      "\u001b[34mEpoch: 4, Loss: 0.6821203351020813\u001b[0m\n",
      "\u001b[34mEpoch: 5, Loss: 0.6821073293685913\u001b[0m\n",
      "\u001b[34mEpoch: 6, Loss: 0.680962347984314\u001b[0m\n",
      "\u001b[34mEpoch: 7, Loss: 0.6808705568313599\u001b[0m\n",
      "\u001b[34mEpoch: 8, Loss: 0.6810302495956421\u001b[0m\n",
      "\u001b[34mEpoch: 9, Loss: 0.6817644596099853\u001b[0m\n",
      "\u001b[34mEpoch: 10, Loss: 0.6807276487350464\u001b[0m\n",
      "\u001b[34mEpoch: 11, Loss: 0.680952537059784\u001b[0m\n",
      "\u001b[34mEpoch: 12, Loss: 0.6810768246650696\u001b[0m\n",
      "\u001b[34mEpoch: 13, Loss: 0.6809253931045532\u001b[0m\n",
      "\u001b[34mEpoch: 14, Loss: 0.6806733846664429\u001b[0m\n",
      "\u001b[34mEpoch: 15, Loss: 0.6810461997985839\u001b[0m\n",
      "\u001b[34mEpoch: 16, Loss: 0.6812384128570557\u001b[0m\n",
      "\u001b[34mEpoch: 17, Loss: 0.6806734800338745\u001b[0m\n",
      "\u001b[34mEpoch: 18, Loss: 0.6805457711219788\u001b[0m\n",
      "\u001b[34mEpoch: 19, Loss: 0.6805558323860168\u001b[0m\n",
      "\u001b[34mEpoch: 20, Loss: 0.6807310461997986\u001b[0m\n",
      "\u001b[34mEpoch: 21, Loss: 0.6804652690887452\u001b[0m\n",
      "\u001b[34mEpoch: 22, Loss: 0.6805539846420288\u001b[0m\n",
      "\u001b[34mEpoch: 23, Loss: 0.6805642485618592\u001b[0m\n",
      "\u001b[34mEpoch: 24, Loss: 0.680442500114441\u001b[0m\n",
      "\u001b[34mEpoch: 25, Loss: 0.6808114051818848\u001b[0m\n",
      "\u001b[34mEpoch: 26, Loss: 0.6806506633758544\u001b[0m\n",
      "\u001b[34mEpoch: 27, Loss: 0.6802897095680237\u001b[0m\n",
      "\u001b[34mEpoch: 28, Loss: 0.6805866837501526\u001b[0m\n",
      "\u001b[34mEpoch: 29, Loss: 0.6803527116775513\u001b[0m\n",
      "\u001b[34mEpoch: 30, Loss: 0.6804935693740845\u001b[0m\n",
      "\u001b[34mEpoch: 31, Loss: 0.6805758714675904\u001b[0m\n",
      "\u001b[34mEpoch: 32, Loss: 0.680451762676239\u001b[0m\n",
      "\u001b[34mEpoch: 33, Loss: 0.6802388310432435\u001b[0m\n",
      "\u001b[34mEpoch: 34, Loss: 0.6800971746444702\u001b[0m\n",
      "\u001b[34mEpoch: 35, Loss: 0.6803120493888855\u001b[0m\n",
      "\u001b[34mEpoch: 36, Loss: 0.6803571701049804\u001b[0m\n",
      "\u001b[34mEpoch: 37, Loss: 0.6803308963775635\u001b[0m\n",
      "\u001b[34mEpoch: 38, Loss: 0.6802764415740967\u001b[0m\n",
      "\u001b[34mEpoch: 39, Loss: 0.6801685333251953\u001b[0m\n",
      "\u001b[34mEpoch: 40, Loss: 0.6802808165550231\u001b[0m\n",
      "\u001b[34mEpoch: 41, Loss: 0.6799487829208374\u001b[0m\n",
      "\u001b[34mEpoch: 42, Loss: 0.6802057147026062\u001b[0m\n",
      "\u001b[34mEpoch: 43, Loss: 0.6804549813270568\u001b[0m\n",
      "\u001b[34mEpoch: 44, Loss: 0.6802473425865173\u001b[0m\n",
      "\u001b[34mEpoch: 45, Loss: 0.6802175879478455\u001b[0m\n",
      "\u001b[34mEpoch: 46, Loss: 0.6802164316177368\u001b[0m\n",
      "\u001b[34mEpoch: 47, Loss: 0.680228078365326\u001b[0m\n",
      "\u001b[34mEpoch: 48, Loss: 0.680115532875061\u001b[0m\n",
      "\u001b[34mEpoch: 49, Loss: 0.680240023136139\u001b[0m\n",
      "\u001b[34mEpoch: 50, Loss: 0.6800470471382141\u001b[0m\n",
      "\u001b[34mEpoch: 51, Loss: 0.6799119591712952\u001b[0m\n",
      "\u001b[34mEpoch: 52, Loss: 0.6802372336387634\u001b[0m\n",
      "\u001b[34mEpoch: 53, Loss: 0.6798864841461182\u001b[0m\n",
      "\u001b[34mEpoch: 54, Loss: 0.6798965811729432\u001b[0m\n",
      "\u001b[34mEpoch: 55, Loss: 0.6800995707511902\u001b[0m\n",
      "\u001b[34mEpoch: 56, Loss: 0.6800467014312744\u001b[0m\n",
      "\u001b[34mEpoch: 57, Loss: 0.6802149057388306\u001b[0m\n",
      "\u001b[34mEpoch: 58, Loss: 0.6799448132514954\u001b[0m\n",
      "\u001b[34mEpoch: 59, Loss: 0.6798201560974121\u001b[0m\n",
      "\u001b[34mEpoch: 60, Loss: 0.6798583626747131\u001b[0m\n",
      "\u001b[34mEpoch: 61, Loss: 0.6799494624137878\u001b[0m\n",
      "\u001b[34mEpoch: 62, Loss: 0.6799867153167725\u001b[0m\n",
      "\u001b[34mEpoch: 63, Loss: 0.6799303531646729\u001b[0m\n",
      "\u001b[34mEpoch: 64, Loss: 0.680062448978424\u001b[0m\n",
      "\u001b[34mEpoch: 65, Loss: 0.6800683021545411\u001b[0m\n",
      "\u001b[34mEpoch: 66, Loss: 0.6799746036529541\u001b[0m\n",
      "\u001b[34mEpoch: 67, Loss: 0.6800879836082458\u001b[0m\n",
      "\u001b[34mEpoch: 68, Loss: 0.6799296736717224\u001b[0m\n",
      "\u001b[34mEpoch: 69, Loss: 0.6800167918205261\u001b[0m\n",
      "\u001b[34mEpoch: 70, Loss: 0.6799410104751586\u001b[0m\n",
      "\u001b[34mEpoch: 71, Loss: 0.6798456311225891\u001b[0m\n",
      "\u001b[34mEpoch: 72, Loss: 0.6799381494522094\u001b[0m\n",
      "\u001b[34mEpoch: 73, Loss: 0.6798555970191955\u001b[0m\n",
      "\u001b[34mEpoch: 74, Loss: 0.679700779914856\u001b[0m\n",
      "\u001b[34mEpoch: 75, Loss: 0.6798343300819397\u001b[0m\n",
      "\u001b[34mEpoch: 76, Loss: 0.6799136400222778\u001b[0m\n",
      "\u001b[34mEpoch: 77, Loss: 0.6798754692077636\u001b[0m\n",
      "\u001b[34mEpoch: 78, Loss: 0.6798246383666993\u001b[0m\n",
      "\u001b[34mEpoch: 79, Loss: 0.6799384713172912\u001b[0m\n",
      "\u001b[34mEpoch: 80, Loss: 0.6794180989265441\u001b[0m\n",
      "\u001b[34mEpoch: 81, Loss: 0.6797626137733459\u001b[0m\n",
      "\u001b[34mEpoch: 82, Loss: 0.680078125\u001b[0m\n",
      "\u001b[34mEpoch: 83, Loss: 0.679781687259674\u001b[0m\n",
      "\u001b[34mEpoch: 84, Loss: 0.6796175718307496\u001b[0m\n",
      "\u001b[34mEpoch: 85, Loss: 0.6794852018356323\u001b[0m\n",
      "\u001b[34mEpoch: 86, Loss: 0.6798584938049317\u001b[0m\n",
      "\u001b[34mEpoch: 87, Loss: 0.6797947525978089\u001b[0m\n",
      "\u001b[34mEpoch: 88, Loss: 0.6798493027687073\u001b[0m\n",
      "\u001b[34mEpoch: 89, Loss: 0.6795132040977478\u001b[0m\n",
      "\u001b[34mEpoch: 90, Loss: 0.6798369407653808\u001b[0m\n",
      "\u001b[34mEpoch: 91, Loss: 0.6795907378196716\u001b[0m\n",
      "\u001b[34mEpoch: 92, Loss: 0.6797283887863159\u001b[0m\n",
      "\u001b[34mEpoch: 93, Loss: 0.679750406742096\u001b[0m\n",
      "\u001b[34mEpoch: 94, Loss: 0.6797082424163818\u001b[0m\n",
      "\u001b[34mEpoch: 95, Loss: 0.6796640753746033\u001b[0m\n",
      "\u001b[34mEpoch: 96, Loss: 0.6795077443122863\u001b[0m\n",
      "\u001b[34mEpoch: 97, Loss: 0.6798348784446716\u001b[0m\n",
      "\u001b[34mEpoch: 98, Loss: 0.6795486688613892\u001b[0m\n",
      "\u001b[34mEpoch: 99, Loss: 0.6797577261924743\u001b[0m\n",
      "\u001b[34mEpoch: 100, Loss: 0.6795331478118897\u001b[0m\n",
      "\u001b[34mEpoch: 101, Loss: 0.679818069934845\u001b[0m\n",
      "\u001b[34mEpoch: 102, Loss: 0.6796770453453064\u001b[0m\n",
      "\u001b[34mEpoch: 103, Loss: 0.6795653343200684\u001b[0m\n",
      "\u001b[34mEpoch: 104, Loss: 0.6797059178352356\u001b[0m\n",
      "\u001b[34mEpoch: 105, Loss: 0.6794815301895142\u001b[0m\n",
      "\u001b[34mEpoch: 106, Loss: 0.6796689867973328\u001b[0m\n",
      "\u001b[34mEpoch: 107, Loss: 0.6792234301567077\u001b[0m\n",
      "\u001b[34mEpoch: 108, Loss: 0.6793176531791687\u001b[0m\n",
      "\u001b[34mEpoch: 109, Loss: 0.6794133424758911\u001b[0m\n",
      "\u001b[34mEpoch: 110, Loss: 0.679741358757019\u001b[0m\n",
      "\u001b[34mEpoch: 111, Loss: 0.6794386744499207\u001b[0m\n",
      "\u001b[34mEpoch: 112, Loss: 0.6793972492218018\u001b[0m\n",
      "\u001b[34mEpoch: 113, Loss: 0.6794689416885376\u001b[0m\n",
      "\u001b[34mEpoch: 114, Loss: 0.6793805956840515\u001b[0m\n",
      "\u001b[34mEpoch: 115, Loss: 0.679642629623413\u001b[0m\n",
      "\u001b[34mEpoch: 116, Loss: 0.6795000314712525\u001b[0m\n",
      "\u001b[34mEpoch: 117, Loss: 0.6796700477600097\u001b[0m\n",
      "\u001b[34mEpoch: 118, Loss: 0.6793842315673828\u001b[0m\n",
      "\u001b[34mEpoch: 119, Loss: 0.6795628070831299\u001b[0m\n",
      "\u001b[34mEpoch: 120, Loss: 0.6797388672828675\u001b[0m\n",
      "\u001b[34mEpoch: 121, Loss: 0.6793587684631348\u001b[0m\n",
      "\u001b[34mEpoch: 122, Loss: 0.6794462323188781\u001b[0m\n",
      "\u001b[34mEpoch: 123, Loss: 0.6791805863380432\u001b[0m\n",
      "\u001b[34mEpoch: 124, Loss: 0.6794160723686218\u001b[0m\n",
      "\u001b[34mEpoch: 125, Loss: 0.6792091012001038\u001b[0m\n",
      "\u001b[34mEpoch: 126, Loss: 0.6793216347694397\u001b[0m\n",
      "\u001b[34mEpoch: 127, Loss: 0.679575252532959\u001b[0m\n",
      "\u001b[34mEpoch: 128, Loss: 0.6792884230613708\u001b[0m\n",
      "\u001b[34mEpoch: 129, Loss: 0.6794373393058777\u001b[0m\n",
      "\u001b[34mEpoch: 130, Loss: 0.6794402003288269\u001b[0m\n",
      "\u001b[34mEpoch: 131, Loss: 0.6793870329856873\u001b[0m\n",
      "\u001b[34mEpoch: 132, Loss: 0.6795890688896179\u001b[0m\n",
      "\u001b[34mEpoch: 133, Loss: 0.6792001962661743\u001b[0m\n",
      "\u001b[34mEpoch: 134, Loss: 0.6792064785957337\u001b[0m\n",
      "\u001b[34mEpoch: 135, Loss: 0.6791101813316345\u001b[0m\n",
      "\u001b[34mEpoch: 136, Loss: 0.679106080532074\u001b[0m\n",
      "\u001b[34mEpoch: 137, Loss: 0.6792595505714416\u001b[0m\n",
      "\u001b[34mEpoch: 138, Loss: 0.679229986667633\u001b[0m\n",
      "\u001b[34mEpoch: 139, Loss: 0.6792447447776795\u001b[0m\n",
      "\u001b[34mEpoch: 140, Loss: 0.6790482044219971\u001b[0m\n",
      "\u001b[34mEpoch: 141, Loss: 0.678838574886322\u001b[0m\n",
      "\u001b[34mEpoch: 142, Loss: 0.679282009601593\u001b[0m\n",
      "\u001b[34mEpoch: 143, Loss: 0.6791936039924622\u001b[0m\n",
      "\u001b[34mEpoch: 144, Loss: 0.6793080925941467\u001b[0m\n",
      "\u001b[34mEpoch: 145, Loss: 0.679070234298706\u001b[0m\n",
      "\u001b[34mEpoch: 146, Loss: 0.6795040607452393\u001b[0m\n",
      "\u001b[34mEpoch: 147, Loss: 0.6791311025619506\u001b[0m\n",
      "\u001b[34mEpoch: 148, Loss: 0.6792758226394653\u001b[0m\n",
      "\u001b[34mEpoch: 149, Loss: 0.6791523933410645\u001b[0m\n",
      "\u001b[34mEpoch: 150, Loss: 0.6793208479881286\u001b[0m\n",
      "\u001b[34mEpoch: 151, Loss: 0.6790526032447814\u001b[0m\n",
      "\u001b[34mEpoch: 152, Loss: 0.6790998101234436\u001b[0m\n",
      "\u001b[34mEpoch: 153, Loss: 0.6790125012397766\u001b[0m\n",
      "\u001b[34mEpoch: 154, Loss: 0.6790276765823364\u001b[0m\n",
      "\u001b[34mEpoch: 155, Loss: 0.6791544675827026\u001b[0m\n",
      "\u001b[34mEpoch: 156, Loss: 0.6789897203445434\u001b[0m\n",
      "\u001b[34mEpoch: 157, Loss: 0.6794138073921203\u001b[0m\n",
      "\u001b[34mEpoch: 158, Loss: 0.6790760636329651\u001b[0m\n",
      "\u001b[34mEpoch: 159, Loss: 0.6792074799537658\u001b[0m\n",
      "\u001b[34mEpoch: 160, Loss: 0.6790085196495056\u001b[0m\n",
      "\u001b[34mEpoch: 161, Loss: 0.6787999868392944\u001b[0m\n",
      "\u001b[34mEpoch: 162, Loss: 0.6793811678886413\u001b[0m\n",
      "\u001b[34mEpoch: 163, Loss: 0.6792029976844788\u001b[0m\n",
      "\u001b[34mEpoch: 164, Loss: 0.6793107628822327\u001b[0m\n",
      "\u001b[34mEpoch: 165, Loss: 0.6790743350982666\u001b[0m\n",
      "\u001b[34mEpoch: 166, Loss: 0.6789887547492981\u001b[0m\n",
      "\u001b[34mEpoch: 167, Loss: 0.6794380187988281\u001b[0m\n",
      "\u001b[34mEpoch: 168, Loss: 0.6792261838912964\u001b[0m\n",
      "\u001b[34mEpoch: 169, Loss: 0.6791001796722412\u001b[0m\n",
      "\u001b[34mEpoch: 170, Loss: 0.6788407802581787\u001b[0m\n",
      "\u001b[34mEpoch: 171, Loss: 0.678823983669281\u001b[0m\n",
      "\u001b[34mEpoch: 172, Loss: 0.6788717269897461\u001b[0m\n",
      "\u001b[34mEpoch: 173, Loss: 0.6790203213691711\u001b[0m\n",
      "\u001b[34mEpoch: 174, Loss: 0.6791786313056946\u001b[0m\n",
      "\u001b[34mEpoch: 175, Loss: 0.6789018511772156\u001b[0m\n",
      "\u001b[34mEpoch: 176, Loss: 0.6791623115539551\u001b[0m\n",
      "\u001b[34mEpoch: 177, Loss: 0.6789417505264282\u001b[0m\n",
      "\u001b[34mEpoch: 178, Loss: 0.6789714097976685\u001b[0m\n",
      "\u001b[34mEpoch: 179, Loss: 0.6790468573570252\u001b[0m\n",
      "\u001b[34mEpoch: 180, Loss: 0.6791730523109436\u001b[0m\n",
      "\u001b[34mEpoch: 181, Loss: 0.6791828513145447\u001b[0m\n",
      "\u001b[34mEpoch: 182, Loss: 0.6790576815605164\u001b[0m\n",
      "\u001b[34mEpoch: 183, Loss: 0.6792127370834351\u001b[0m\n",
      "\u001b[34mEpoch: 184, Loss: 0.6787413239479065\u001b[0m\n",
      "\u001b[34mEpoch: 185, Loss: 0.679174256324768\u001b[0m\n",
      "\u001b[34mEpoch: 186, Loss: 0.6787226080894471\u001b[0m\n",
      "\u001b[34mEpoch: 187, Loss: 0.6788108229637146\u001b[0m\n",
      "\u001b[34mEpoch: 188, Loss: 0.678868317604065\u001b[0m\n",
      "\u001b[34mEpoch: 189, Loss: 0.67864830493927\u001b[0m\n",
      "\u001b[34mEpoch: 190, Loss: 0.6786625623703003\u001b[0m\n",
      "\u001b[34mEpoch: 191, Loss: 0.6790469765663147\u001b[0m\n",
      "\u001b[34mEpoch: 192, Loss: 0.6789719700813294\u001b[0m\n",
      "\u001b[34mEpoch: 193, Loss: 0.6787145972251892\u001b[0m\n",
      "\u001b[34mEpoch: 194, Loss: 0.6790199637413025\u001b[0m\n",
      "\u001b[34mEpoch: 195, Loss: 0.6788477540016175\u001b[0m\n",
      "\u001b[34mEpoch: 196, Loss: 0.6788549184799194\u001b[0m\n",
      "\u001b[34mEpoch: 197, Loss: 0.6790143013000488\u001b[0m\n",
      "\u001b[34mEpoch: 198, Loss: 0.6784821152687073\u001b[0m\n",
      "\u001b[34mEpoch: 199, Loss: 0.6790027856826782\u001b[0m\n",
      "\u001b[34mEpoch: 200, Loss: 0.6787465572357178\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34m2020-02-22 22:56:57,666 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-02-22 22:57:10 Uploading - Uploading generated training model\n",
      "2020-02-22 22:57:10 Completed - Training job completed\n",
      "Training seconds: 54\n",
      "Billable seconds: 54\n",
      "-----------------!Processing: ABB\n",
      "2020-02-22 23:06:10 Starting - Starting the training job...\n",
      "2020-02-22 23:06:11 Starting - Launching requested ML instances......\n",
      "2020-02-22 23:07:35 Starting - Preparing the instances for training............\n",
      "2020-02-22 23:09:19 Downloading - Downloading input data...\n",
      "2020-02-22 23:10:07 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'_enable_profiler': u'false', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'local_lloyd_num_trials': u'auto', u'_log_level': u'info', u'_kvstore': u'auto', u'local_lloyd_init_method': u'kmeans++', u'force_dense': u'true', u'epochs': u'1', u'init_method': u'random', u'local_lloyd_tol': u'0.0001', u'local_lloyd_max_iter': u'300', u'_disable_wait_to_read': u'false', u'extra_center_factor': u'auto', u'eval_metrics': u'[\"msd\"]', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'half_life_time_size': u'0', u'_num_slices': u'1'}\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'feature_dim': u'26', u'k': u'3', u'force_dense': u'True'}\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] Final configuration: {u'_tuning_objective_metric': u'', u'extra_center_factor': u'auto', u'local_lloyd_init_method': u'kmeans++', u'force_dense': u'True', u'epochs': u'1', u'feature_dim': u'26', u'local_lloyd_tol': u'0.0001', u'_disable_wait_to_read': u'false', u'eval_metrics': u'[\"msd\"]', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'_enable_profiler': u'false', u'_num_gpus': u'auto', u'local_lloyd_num_trials': u'auto', u'_log_level': u'info', u'init_method': u'random', u'half_life_time_size': u'0', u'local_lloyd_max_iter': u'300', u'_kvstore': u'auto', u'k': u'3', u'_num_slices': u'1'}\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 WARNING 140249624278848] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] Using default worker.\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] Loaded iterator creator application/x-recordio-protobuf for content type ('application/x-recordio-protobuf', '1.0')\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] Create Store: local\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] nvidia-smi took: 0.0252242088318 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] Setting up with params: {u'_tuning_objective_metric': u'', u'extra_center_factor': u'auto', u'local_lloyd_init_method': u'kmeans++', u'force_dense': u'True', u'epochs': u'1', u'feature_dim': u'26', u'local_lloyd_tol': u'0.0001', u'_disable_wait_to_read': u'false', u'eval_metrics': u'[\"msd\"]', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'_enable_profiler': u'false', u'_num_gpus': u'auto', u'local_lloyd_num_trials': u'auto', u'_log_level': u'info', u'init_method': u'random', u'half_life_time_size': u'0', u'local_lloyd_max_iter': u'300', u'_kvstore': u'auto', u'k': u'3', u'_num_slices': u'1'}\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] 'extra_center_factor' was set to 'auto', evaluated to 10.\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] number of center slices 1\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 WARNING 140249624278848] Batch size 5000 is bigger than the first batch data. Effective batch size used to initialize is 1662\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1662, \"sum\": 1662.0, \"min\": 1662}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1662, \"sum\": 1662.0, \"min\": 1662}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1662, \"sum\": 1662.0, \"min\": 1662}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1582413010.24034, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\"}, \"StartTime\": 1582413010.240283}\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-02-22 23:10:10.240] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 22, \"num_examples\": 1, \"num_bytes\": 212736}\u001b[0m\n",
      "\u001b[34m[2020-02-22 23:10:10.273] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 32, \"num_examples\": 1, \"num_bytes\": 212736}\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] processed a total of 1662 examples\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1662, \"sum\": 1662.0, \"min\": 1662}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Total Records Seen\": {\"count\": 1, \"max\": 3324, \"sum\": 3324.0, \"min\": 3324}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1662, \"sum\": 1662.0, \"min\": 1662}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1582413010.274528, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\", \"epoch\": 0}, \"StartTime\": 1582413010.240681}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] #throughput_metric: host=algo-1, train throughput=48868.0755987 records/second\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 WARNING 140249624278848] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] shrinking 30 centers into 3\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] local kmeans attempt #0. Current mean square distance 0.257578\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] local kmeans attempt #1. Current mean square distance 0.259286\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] local kmeans attempt #2. Current mean square distance 0.295160\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] local kmeans attempt #3. Current mean square distance 0.264561\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] local kmeans attempt #4. Current mean square distance 0.284269\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] local kmeans attempt #5. Current mean square distance 0.285979\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] local kmeans attempt #6. Current mean square distance 0.285998\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] local kmeans attempt #7. Current mean square distance 0.333886\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] local kmeans attempt #8. Current mean square distance 0.254377\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] local kmeans attempt #9. Current mean square distance 0.266053\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] finished shrinking process. Mean Square Distance = 0\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] #quality_metric: host=algo-1, train msd <loss>=0.254377067089\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] compute all data-center distances: inner product took: 23.4402%, (0.009595 secs)\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] collect from kv store took: 19.8785%, (0.008137 secs)\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] splitting centers key-value pair took: 19.6298%, (0.008035 secs)\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] batch data loading with context took: 8.5527%, (0.003501 secs)\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] gradient: cluster center took: 7.6167%, (0.003118 secs)\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] compute all data-center distances: point norm took: 7.6150%, (0.003117 secs)\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] gradient: one_hot took: 5.1005%, (0.002088 secs)\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] predict compute msd took: 4.6101%, (0.001887 secs)\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] update state and report convergance took: 1.3094%, (0.000536 secs)\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] gradient: cluster size  took: 1.0845%, (0.000444 secs)\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] update set-up time took: 0.5836%, (0.000239 secs)\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] compute all data-center distances: center norm took: 0.5131%, (0.000210 secs)\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] predict minus dist took: 0.0658%, (0.000027 secs)\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] TOTAL took: 0.0409336090088\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 146.38209342956543, \"sum\": 146.38209342956543, \"min\": 146.38209342956543}, \"initialize.time\": {\"count\": 1, \"max\": 18.618106842041016, \"sum\": 18.618106842041016, \"min\": 18.618106842041016}, \"model.serialize.time\": {\"count\": 1, \"max\": 0.1621246337890625, \"sum\": 0.1621246337890625, \"min\": 0.1621246337890625}, \"update.time\": {\"count\": 1, \"max\": 33.555030822753906, \"sum\": 33.555030822753906, \"min\": 33.555030822753906}, \"epochs\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"state.serialize.time\": {\"count\": 1, \"max\": 0.9548664093017578, \"sum\": 0.9548664093017578, \"min\": 0.9548664093017578}, \"_shrink.time\": {\"count\": 1, \"max\": 144.45090293884277, \"sum\": 144.45090293884277, \"min\": 144.45090293884277}}, \"EndTime\": 1582413010.422612, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\"}, \"StartTime\": 1582413010.217011}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/22/2020 23:10:10 INFO 140249624278848] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 273.2360363006592, \"sum\": 273.2360363006592, \"min\": 273.2360363006592}, \"setuptime\": {\"count\": 1, \"max\": 14.922142028808594, \"sum\": 14.922142028808594, \"min\": 14.922142028808594}}, \"EndTime\": 1582413010.423011, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\"}, \"StartTime\": 1582413010.422723}\n",
      "\u001b[0m\n",
      "\n",
      "2020-02-22 23:10:20 Uploading - Uploading generated training model\n",
      "2020-02-22 23:10:20 Completed - Training job completed\n",
      "Training seconds: 61\n",
      "Billable seconds: 61\n",
      "Deploying model s3://sagemaker-us-west-2-825285592721/udacity-capstone-project/output/kmeans-2020-02-22-23-06-09-881/output/model.tar.gz\n",
      "-----------------!2020-02-22 23:19:25 Starting - Starting the training job...\n",
      "2020-02-22 23:19:27 Starting - Launching requested ML instances...\n",
      "2020-02-22 23:20:24 Starting - Preparing the instances for training......\n",
      "2020-02-22 23:21:26 Downloading - Downloading input data\n",
      "2020-02-22 23:21:26 Training - Downloading the training image."
     ]
    }
   ],
   "source": [
    "# tickers = list(map(lambda x: x.replace(local_data_folder + '/', '').replace('.csv', ''), glob(local_data_folder + \"/*.csv\")))\n",
    "\n",
    "# ValueError: Classification metrics can't handle a mix of multiclass and continuous targets\n",
    "problematic_tickers = [\"AA\"]\n",
    "tickers = [\"AAP\", \"ABB\"]\n",
    "\n",
    "for ticker in tickers:\n",
    "    try :\n",
    "        print('Processing:', ticker)\n",
    "        process(ticker)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
