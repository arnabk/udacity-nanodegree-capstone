{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker import KMeans\n",
    "import os\n",
    "import numpy as np\n",
    "from helper.utils import create_dir, clustering, save_data\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from glob import glob\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "threshold = .4\n",
    "BUY = 1\n",
    "SELL = 2\n",
    "NONE = 3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "local_data_folder = './data'\n",
    "prefix = \"udacity-capstone-project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir):\n",
    "  os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "# Generate clusters for data\n",
    "def clustering(data, kmeans_predictor):\n",
    "    clustering_result = kmeans_predictor.predict(pd.DataFrame(data).astype('float32').values)\n",
    "    clustering_result = list(map(lambda x:x.label[\"closest_cluster\"].float32_tensor.values[0], clustering_result))\n",
    "\n",
    "    assert len(clustering_result) == len(data), \"Length mis-match with clustering and input data\"\n",
    "\n",
    "    cluster_category = pd.DataFrame(clustering_result, columns=[\"Cluster\"])\n",
    "    x_train_with_cluster = pd.concat([pd.DataFrame(data), cluster_category], axis=1)\n",
    "    return cluster_category\n",
    "\n",
    "# save data to local dir\n",
    "def save_data(cluster_data, folder_name):\n",
    "    Y = cluster_data[[\"Label\"]]\n",
    "    X = cluster_data.drop(columns=[\"Label\"])\n",
    "    create_dir(local_data_folder + '/s3/' + folder_name)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.33, random_state=1, shuffle=True)\n",
    "    pd.concat([pd.DataFrame(y_train), pd.DataFrame(x_train)], axis=1)\\\n",
    "        .to_csv(local_data_folder + '/s3/' + folder_name + '/train.csv', header=False, index=False)\n",
    "    pd.concat([pd.DataFrame(y_test), pd.DataFrame(x_test)], axis=1)\\\n",
    "        .to_csv(local_data_folder + '/s3/' + folder_name + '/validation.csv', header=False, index=False)\n",
    "        \n",
    "def generate_NN_predictor(ticker):\n",
    "    s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/data/{}/train.csv'\\\n",
    "                                        .format(bucket, prefix, ticker), content_type='text/csv')\n",
    "    s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/data/{}/validation.csv'\\\n",
    "                                             .format(bucket, prefix, ticker), content_type='text/csv')\n",
    "    estimator = PyTorch(entry_point='train.py',\n",
    "                        source_dir='pytorch', # this should be just \"source\" for your code\n",
    "                        role=role,\n",
    "                        framework_version='1.0',\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type='ml.c4.xlarge',\n",
    "                        sagemaker_session=sagemaker_session,\n",
    "                        hyperparameters={\n",
    "                            'input_dim': 26,  # num of features\n",
    "                            'hidden_dim': 260,\n",
    "                            'output_dim': 1,\n",
    "                            'epochs': 200 # could change to higher\n",
    "                        })\n",
    "    estimator.fit({ 'train': s3_input_train, 'validation': s3_input_validation })\n",
    "    predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")\n",
    "    return predictor\n",
    "\n",
    "def generate_random_direction():\n",
    "    rand_val = random.random()\n",
    "    direction = NONE\n",
    "    if rand_val >= .7:\n",
    "        direction = BUY\n",
    "    elif rand_val <= .3:\n",
    "        direction = SELL\n",
    "    return direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(ticker):\n",
    "    df = pd.read_pickle('{}/{}.{}'.format(local_data_folder, ticker, 'pkl'))\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop(columns=[\"Date\"], inplace=True)\n",
    "    df.loc[df.Label >= threshold, 'direction'] = BUY\n",
    "    df.loc[df.Label <= -threshold, 'direction'] = SELL\n",
    "    df.loc[(df.Label < threshold) & (df.Label > -threshold), 'direction'] = NONE\n",
    "\n",
    "    # Normalize\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    Y_df = pd.DataFrame(df[\"Label\"]).astype('float64')\n",
    "    X_df = df.drop(columns=[\"Label\"]).astype('float64')\n",
    "\n",
    "    X = scaler.fit_transform(X_df)\n",
    "    Y = scaler.fit_transform(Y_df)\n",
    "\n",
    "    X[:, X.shape[1] - 1] = X_df[\"direction\"].to_numpy()\n",
    "\n",
    "    #### split data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.33, random_state=1, shuffle=True)\n",
    "\n",
    "    # clustering\n",
    "    s3_output_folder = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "    kmeans = KMeans(role=role,\n",
    "                train_instance_count=1,\n",
    "                train_instance_type=\"ml.m4.xlarge\",\n",
    "                output_path=s3_output_folder,\n",
    "                k=3)\n",
    "\n",
    "    # Remove direction column and train\n",
    "    kmeans.fit(kmeans.record_set(x_train[:, 0:x_train.shape[1] - 1].astype('float32')))\n",
    "\n",
    "    # deploy\n",
    "    print(\"Deploying model\", kmeans.model_data)\n",
    "    kmeans_predictor = kmeans.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")\n",
    "\n",
    "    create_dir('{}/s3/{}'.format(local_data_folder, ticker))\n",
    "\n",
    "    '''\n",
    "        Label = Change in price(+ve, -ve, none)\n",
    "        Direction = BUY, SELL, NONE\n",
    "        Cluster = cluster_0, cluster_1, cluster_2\n",
    "    '''\n",
    "    # train data\n",
    "    y_train_df = pd.DataFrame(y_train, columns=[\"Label\"])\n",
    "    x_train_df = pd.DataFrame(x_train, columns=['col-{}'.format(i) for i in range(x_train.shape[1] - 1)] + [\"direction\"])\n",
    "    dataset_with_cluster = pd.concat([y_train_df.astype(\"float32\"), x_train_df.astype(\"float32\"),\\\n",
    "            clustering(x_train_df.drop(columns=[\"direction\"]).astype('float32').values, kmeans_predictor)\n",
    "        ], axis=1)\n",
    "    dataset_with_cluster.to_csv('{}/s3/{}/all-train.csv'.format(local_data_folder, ticker), header=True, index=False)\n",
    "\n",
    "    # test data\n",
    "    y_test_df = pd.DataFrame(y_test, columns=[\"Label\"])\n",
    "    x_test_df = pd.DataFrame(x_test, columns=['col-{}'.format(i) for i in range(x_test.shape[1] - 1)] + ['direction'])\n",
    "    pd.concat([y_test_df.astype(\"float32\"), x_test_df.astype(\"float32\")], axis=1)\\\n",
    "        .to_csv('{}/s3/{}/all-test.csv'.format(local_data_folder, ticker), header=True, index=False)\n",
    "\n",
    "    # clean clustering end point\n",
    "    kmeans_predictor.delete_endpoint(kmeans_predictor.endpoint)\n",
    "\n",
    "    all_test_pred = pd.read_csv(\"{}/s3/{}/all-test.csv\".format(local_data_folder, ticker))\n",
    "    all_train_pred = pd.read_csv(\"{}/s3/{}/all-train.csv\".format(local_data_folder, ticker))\n",
    "\n",
    "    cluster0_df = dataset_with_cluster[dataset_with_cluster[\"Cluster\"] == 0].drop(columns=[\"Cluster\"])\n",
    "    save_data(cluster0_df.drop(columns=[\"direction\"]), ticker)\n",
    "    sagemaker_session.upload_data(path=local_data_folder + '/s3/' + ticker, bucket=bucket, key_prefix=prefix + '/data/' + ticker)\n",
    "    estimator = generate_NN_predictor(ticker)\n",
    "    all_test_pred[\"cluster0_pred\"] = estimator.predict(all_test_pred.drop(columns=[\"Label\", \"direction\"]).astype('float32').values)\n",
    "    all_train_pred[\"cluster0_pred\"] = estimator.predict(all_train_pred.drop(columns=[\"Label\", \"direction\", \"Cluster\"]).astype('float32').values)\n",
    "    estimator.delete_endpoint(estimator.endpoint)\n",
    "\n",
    "    cluster1_df = dataset_with_cluster[dataset_with_cluster[\"Cluster\"] == 1].drop(columns=[\"Cluster\"])\n",
    "    save_data(cluster1_df.drop(columns=[\"direction\"]), ticker)\n",
    "    sagemaker_session.upload_data(path=local_data_folder + '/s3/' + ticker, bucket=bucket, key_prefix=prefix + '/data/' + ticker)\n",
    "    estimator = generate_NN_predictor(ticker)\n",
    "    all_test_pred[\"cluster1_pred\"] = estimator.predict(all_test_pred.drop(columns=[\"Label\", \"direction\", \"cluster0_pred\"]).astype('float32').values)\n",
    "    all_train_pred[\"cluster1_pred\"] = estimator.predict(all_train_pred.drop(columns=[\"Label\", \"direction\", \"Cluster\", \"cluster0_pred\"]).astype('float32').values)\n",
    "    estimator.delete_endpoint(estimator.endpoint)\n",
    "\n",
    "    cluster2_df = dataset_with_cluster[dataset_with_cluster[\"Cluster\"] == 2].drop(columns=[\"Cluster\"])\n",
    "    save_data(cluster2_df.drop(columns=[\"direction\"]), ticker)\n",
    "    sagemaker_session.upload_data(path=local_data_folder + '/s3/' + ticker, bucket=bucket, key_prefix=prefix + '/data/' + ticker)\n",
    "    estimator = generate_NN_predictor(ticker)\n",
    "    all_test_pred[\"cluster2_pred\"] = estimator.predict(all_test_pred.drop(columns=[\"Label\", \"direction\", \"cluster0_pred\", \"cluster1_pred\"]).astype('float32').values)\n",
    "    all_train_pred[\"cluster2_pred\"] = estimator.predict(all_train_pred.drop(columns=[\"Label\", \"direction\", \"Cluster\", \"cluster0_pred\", \"cluster1_pred\"]).astype('float32').values)\n",
    "    estimator.delete_endpoint(estimator.endpoint)\n",
    "\n",
    "    os.remove(local_data_folder + '/s3/' + ticker + '/train.csv')\n",
    "    os.remove(local_data_folder + '/s3/' + ticker + '/validation.csv')\n",
    "\n",
    "    all_buys = pd.DataFrame([cluster0_df[cluster0_df['direction'] == BUY].shape[0],\n",
    "            cluster1_df[cluster1_df['direction'] == BUY].shape[0],\n",
    "            cluster2_df[cluster2_df['direction'] == BUY].shape[0]], columns=[\"BUY\"], index=[\"cluster0_pred\", \"cluster1_pred\", \"cluster2_pred\"])\n",
    "\n",
    "    all_sells = pd.DataFrame([cluster0_df[cluster0_df['direction'] == SELL].shape[0],\n",
    "            cluster1_df[cluster1_df['direction'] == SELL].shape[0],\n",
    "            cluster2_df[cluster2_df['direction'] == SELL].shape[0]], columns=[\"SELL\"], index=[\"cluster0_pred\", \"cluster1_pred\", \"cluster2_pred\"])\n",
    "\n",
    "    all_nones = pd.DataFrame([cluster0_df[cluster0_df['direction'] == NONE].shape[0],\n",
    "            cluster1_df[cluster1_df['direction'] == NONE].shape[0],\n",
    "            cluster2_df[cluster2_df['direction'] == NONE].shape[0]], columns=[\"NONE\"], index=[\"cluster0_pred\", \"cluster1_pred\", \"cluster2_pred\"])\n",
    "\n",
    "    cluster_selection_df = pd.concat([all_buys, all_sells, all_nones], axis=1)\n",
    "\n",
    "\n",
    "    cluster_selection_index = cluster_selection_df.index\n",
    "    buy_cluster_name = cluster_selection_index[cluster_selection_df['BUY'].values.argmax()]\n",
    "    sell_cluster_name = cluster_selection_index[cluster_selection_df.drop(index=[buy_cluster_name])['SELL'].values.argmax()]\n",
    "    none_cluster_name = cluster_selection_index[cluster_selection_df.drop(index=[buy_cluster_name, sell_cluster_name])['NONE'].values.argmax()]\n",
    "\n",
    "    # Generate selected-cluster column based on max(cluster0, cluster1, cluster2)\n",
    "    all_test_pred[\"selected-cluster\"] = all_test_pred[[\"cluster0_pred\", \"cluster1_pred\", \"cluster2_pred\"]].idxmax(axis=1)\n",
    "    all_train_pred[\"selected-cluster\"] = all_train_pred[[\"cluster0_pred\", \"cluster1_pred\", \"cluster2_pred\"]].idxmax(axis=1)\n",
    "\n",
    "    # convert selected-cluster to BUY, SELL, NONE\n",
    "    all_test_pred.loc[all_test_pred[\"selected-cluster\"] == buy_cluster_name, \"prediction\"] = BUY\n",
    "    all_test_pred.loc[all_test_pred[\"selected-cluster\"] == sell_cluster_name, \"prediction\"] = SELL\n",
    "    all_test_pred.loc[all_test_pred[\"selected-cluster\"] == none_cluster_name, \"prediction\"] = NONE\n",
    "\n",
    "    all_train_pred.loc[all_train_pred[\"selected-cluster\"] == buy_cluster_name, \"prediction\"] = BUY\n",
    "    all_train_pred.loc[all_train_pred[\"selected-cluster\"] == sell_cluster_name, \"prediction\"] = SELL\n",
    "    all_train_pred.loc[all_train_pred[\"selected-cluster\"] == none_cluster_name, \"prediction\"] = NONE\n",
    "\n",
    "    # Bench mark results\n",
    "    all_test_pred[\"random-prediction\"] = [generate_random_direction() for _ in range(all_test_pred.shape[0])]\n",
    "    all_train_pred[\"random-prediction\"] = [generate_random_direction() for _ in range(all_train_pred.shape[0])]\n",
    "\n",
    "\n",
    "    all_test_pred.to_csv('{}/s3/{}/all-test-pred.csv'.format(local_data_folder, ticker), index=None)\n",
    "    all_train_pred.to_csv('{}/s3/{}/all-train-pred.csv'.format(local_data_folder, ticker), index=None)\n",
    "    cluster_selection_df.to_csv('{}/s3/{}/cluster-selection.csv'.format(local_data_folder, ticker), index=None)\n",
    "\n",
    "    # test accuracy\n",
    "    test_accuracy = accuracy_score(all_test_pred[\"direction\"], all_test_pred[\"prediction\"], normalize=True)\n",
    "    benchmark_test_accuracy = accuracy_score(all_test_pred[\"direction\"], all_test_pred[\"random-prediction\"], normalize=True)\n",
    "    print('Test accuracy:', test_accuracy, \", Benchmark:\", benchmark_test_accuracy)\n",
    "\n",
    "    # train accuracy\n",
    "    train_accuracy = accuracy_score(all_train_pred[\"direction\"], all_train_pred[\"prediction\"], normalize=True)\n",
    "    benchmark_train_accuracy = accuracy_score(all_train_pred[\"direction\"], all_train_pred[\"random-prediction\"], normalize=True)\n",
    "    print('Train accuracy:', train_accuracy, \", Benchmark:\", benchmark_train_accuracy)\n",
    "\n",
    "    accuracy_df = pd.DataFrame([ticker, test_accuracy, benchmark_test_accuracy, train_accuracy, benchmark_train_accuracy]).T\n",
    "    accuracy_df.columns = [\"ticker\", \"test_accuracy\", \"benchmark_test_accuracy\", \"train_accuracy\", \"benchmark_train_accuracy\"]\n",
    "\n",
    "    accuracy_file = \"{}/accuracy.csv\".format(local_data_folder)\n",
    "    header = not os.path.exists(accuracy_file)\n",
    "    accuracy_df.to_csv(accuracy_file, mode=\"a\", header=header, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-17 22:35:51 Starting - Starting the training job...\n",
      "2020-02-17 22:35:52 Starting - Launching requested ML instances...\n",
      "2020-02-17 22:36:50 Starting - Preparing the instances for training.........\n",
      "2020-02-17 22:38:20 Downloading - Downloading input data\n",
      "2020-02-17 22:38:20 Training - Downloading the training image...\n",
      "2020-02-17 22:38:52 Uploading - Uploading generated training model\n",
      "2020-02-17 22:38:52 Completed - Training job completed\n",
      "\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'_enable_profiler': u'false', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'local_lloyd_num_trials': u'auto', u'_log_level': u'info', u'_kvstore': u'auto', u'local_lloyd_init_method': u'kmeans++', u'force_dense': u'true', u'epochs': u'1', u'init_method': u'random', u'local_lloyd_tol': u'0.0001', u'local_lloyd_max_iter': u'300', u'_disable_wait_to_read': u'false', u'extra_center_factor': u'auto', u'eval_metrics': u'[\"msd\"]', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'half_life_time_size': u'0', u'_num_slices': u'1'}\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'feature_dim': u'26', u'k': u'3', u'force_dense': u'True'}\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] Final configuration: {u'_tuning_objective_metric': u'', u'extra_center_factor': u'auto', u'local_lloyd_init_method': u'kmeans++', u'force_dense': u'True', u'epochs': u'1', u'feature_dim': u'26', u'local_lloyd_tol': u'0.0001', u'_disable_wait_to_read': u'false', u'eval_metrics': u'[\"msd\"]', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'_enable_profiler': u'false', u'_num_gpus': u'auto', u'local_lloyd_num_trials': u'auto', u'_log_level': u'info', u'init_method': u'random', u'half_life_time_size': u'0', u'local_lloyd_max_iter': u'300', u'_kvstore': u'auto', u'k': u'3', u'_num_slices': u'1'}\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 WARNING 139833352034112] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] Using default worker.\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] Loaded iterator creator application/x-recordio-protobuf for content type ('application/x-recordio-protobuf', '1.0')\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] Create Store: local\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] nvidia-smi took: 0.025230884552 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] Setting up with params: {u'_tuning_objective_metric': u'', u'extra_center_factor': u'auto', u'local_lloyd_init_method': u'kmeans++', u'force_dense': u'True', u'epochs': u'1', u'feature_dim': u'26', u'local_lloyd_tol': u'0.0001', u'_disable_wait_to_read': u'false', u'eval_metrics': u'[\"msd\"]', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'_enable_profiler': u'false', u'_num_gpus': u'auto', u'local_lloyd_num_trials': u'auto', u'_log_level': u'info', u'init_method': u'random', u'half_life_time_size': u'0', u'local_lloyd_max_iter': u'300', u'_kvstore': u'auto', u'k': u'3', u'_num_slices': u'1'}\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] 'extra_center_factor' was set to 'auto', evaluated to 10.\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] number of center slices 1\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 WARNING 139833352034112] Batch size 5000 is bigger than the first batch data. Effective batch size used to initialize is 1662\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1662, \"sum\": 1662.0, \"min\": 1662}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1662, \"sum\": 1662.0, \"min\": 1662}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1662, \"sum\": 1662.0, \"min\": 1662}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1581979122.306866, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\"}, \"StartTime\": 1581979122.306826}\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-02-17 22:38:42.307] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 212736}\u001b[0m\n",
      "\u001b[34m[2020-02-17 22:38:42.340] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 32, \"num_examples\": 1, \"num_bytes\": 212736}\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] processed a total of 1662 examples\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1662, \"sum\": 1662.0, \"min\": 1662}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Total Records Seen\": {\"count\": 1, \"max\": 3324, \"sum\": 3324.0, \"min\": 3324}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1662, \"sum\": 1662.0, \"min\": 1662}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1581979122.34092, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\", \"epoch\": 0}, \"StartTime\": 1581979122.307168}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] #throughput_metric: host=algo-1, train throughput=48971.7536706 records/second\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 WARNING 139833352034112] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] shrinking 30 centers into 3\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] local kmeans attempt #0. Current mean square distance 0.191335\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] local kmeans attempt #1. Current mean square distance 0.195664\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] local kmeans attempt #2. Current mean square distance 0.191335\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] local kmeans attempt #3. Current mean square distance 0.191335\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] local kmeans attempt #4. Current mean square distance 0.260543\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] local kmeans attempt #5. Current mean square distance 0.191335\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] local kmeans attempt #6. Current mean square distance 0.199767\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] local kmeans attempt #7. Current mean square distance 0.191335\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] local kmeans attempt #8. Current mean square distance 0.191335\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] local kmeans attempt #9. Current mean square distance 0.191335\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] finished shrinking process. Mean Square Distance = 0\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] #quality_metric: host=algo-1, train msd <loss>=0.191334605217\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] compute all data-center distances: inner product took: 22.7339%, (0.009325 secs)\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] collect from kv store took: 20.0008%, (0.008204 secs)\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] splitting centers key-value pair took: 19.7108%, (0.008085 secs)\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] batch data loading with context took: 8.5281%, (0.003498 secs)\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] gradient: cluster center took: 7.6115%, (0.003122 secs)\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] compute all data-center distances: point norm took: 7.1552%, (0.002935 secs)\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] predict compute msd took: 5.4876%, (0.002251 secs)\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] gradient: one_hot took: 5.0441%, (0.002069 secs)\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] gradient: cluster size  took: 1.4142%, (0.000580 secs)\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] update state and report convergance took: 1.2485%, (0.000512 secs)\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] update set-up time took: 0.5952%, (0.000244 secs)\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] compute all data-center distances: center norm took: 0.4144%, (0.000170 secs)\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] predict minus dist took: 0.0558%, (0.000023 secs)\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] TOTAL took: 0.0410182476044\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 198.42219352722168, \"sum\": 198.42219352722168, \"min\": 198.42219352722168}, \"initialize.time\": {\"count\": 1, \"max\": 16.809940338134766, \"sum\": 16.809940338134766, \"min\": 16.809940338134766}, \"model.serialize.time\": {\"count\": 1, \"max\": 0.14710426330566406, \"sum\": 0.14710426330566406, \"min\": 0.14710426330566406}, \"update.time\": {\"count\": 1, \"max\": 33.538103103637695, \"sum\": 33.538103103637695, \"min\": 33.538103103637695}, \"epochs\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"state.serialize.time\": {\"count\": 1, \"max\": 0.9469985961914062, \"sum\": 0.9469985961914062, \"min\": 0.9469985961914062}, \"_shrink.time\": {\"count\": 1, \"max\": 196.90895080566406, \"sum\": 196.90895080566406, \"min\": 196.90895080566406}}, \"EndTime\": 1581979122.540969, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\"}, \"StartTime\": 1581979122.285154}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/17/2020 22:38:42 INFO 139833352034112] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 324.08618927001953, \"sum\": 324.08618927001953, \"min\": 324.08618927001953}, \"setuptime\": {\"count\": 1, \"max\": 15.003204345703125, \"sum\": 15.003204345703125, \"min\": 15.003204345703125}}, \"EndTime\": 1581979122.541356, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\"}, \"StartTime\": 1581979122.541107}\n",
      "\u001b[0m\n",
      "Training seconds: 51\n",
      "Billable seconds: 51\n",
      "Deploying model s3://sagemaker-us-west-2-825285592721/udacity-capstone-project/output/kmeans-2020-02-17-22-35-51-182/output/model.tar.gz\n",
      "--------"
     ]
    }
   ],
   "source": [
    "tickers = list(map(lambda x: x.replace(local_data_folder + '/', '').replace('.csv', ''), glob(local_data_folder + \"/*.csv\")))\n",
    "\n",
    "for ticker in tickers:\n",
    "    process(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
